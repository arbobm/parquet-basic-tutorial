---
title: "Parquet Basics"
subtitle: ''
format:
  html:
    embed-resources: true
    code-fold: true
    code-overflow: wrap
    theme: cosmo
    toc: true
    canonical-url: true
execute: 
  warning: false
  error: false
  message: false
  echo: false
# bibliography: references.bib
lightbox: ## https://quarto.org/docs/output-formats/html-lightbox-figures.html
  match: auto
  effect: fade
  desc-position: right
dpi: 500
---

## Welcome to the fantastic world of Parquet

### What is it?

Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides high performance compression and encoding schemes to handle complex data in bulk and is supported in many programming language and analytics tools^[https://parquet.apache.org/docs/overview/].

<!-- https://www.upsolver.com/blog/apache-parquet-why-use -->


<!-- <!-- https://www.databricks.com/glossary/what-is-parquet --> -->

<!-- ### What is Parquet? -->

<!-- Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. Apache Parquet is designed to be a common interchange format for both batch and interactive workloads. It is similar to other columnar-storage file formats available in Hadoop, namely RCFile and ORC. -->


#### Basic Definition

Apache Parquet is a file format designed to support fast data processing for complex data, with several notable characteristics:

1. **Columnar:** Unlike row-based formats such as CSV or Avro, Apache Parquet is column-oriented – meaning the values of each table column are stored next to each other, rather than those of each record:

![](https://www.upsolver.com/wp-content/uploads/2020/05/Screen-Shot-2020-05-26-at-17.52.58.png)

The key difference between a CSV and Parquet file format is how each one is organized. A Parquet file format is structured by row, with every separate column independently accessible from the rest. Since the data in each column is expected to be of the same type, the parquet file format makes encoding, compressing and optimizing data storage possible.

2. **Open-source:** Parquet is free to use and open source under the Apache Hadoop license, and is compatible with most Hadoop data processing frameworks. To quote the [project website](https://parquet.apache.org/){target="_blank"}, "Apache Parquet is… available to any project… regardless of the choice of data processing framework, data model, or programming language."
3. **Self-describing:** In addition to data, a Parquet file contains metadata including schema and structure. Each file stores both the data and the standards used for accessing each record – making it easier to decouple services that write, store, and read Parquet files.
4. **Binary format:** Parquet file formats store data in binary format, which reduces the overhead of textual representation. It's important to note that Parquet files are not stored in plain text, thus cannot be opened in a text editor.

#### Advantages of Parquet Columnar Storage – Why Should You Use It?

The above characteristics of the Apache Parquet file format create several distinct benefits when it comes to storing and analysing large volumes of data. 

::: {.panel-tabset}

## Compression

File compression is the act of taking a file and making it smaller. In Parquet, compression is performed column by column and it is built to support flexible compression options and extendable encoding schemas per data type – e.g., different encoding can be used for compressing integer and string data.

Parquet data can be compressed using these encoding methods:

* **Dictionary encoding:** this is enabled automatically and dynamically for data with a small number of unique values.
* **Bit packing:** Storage of integers is usually done with dedicated 32 or 64 bits per integer. This allows more efficient storage of small integers.
* **Run length encoding (RLE):** when the same value occurs multiple times, a single value is stored once along with the number of occurrences. Parquet implements a combined version of bit packing and RLE, in which the encoding switches based on which produces the best compression results.

<!--     Data compression -->

<!-- Parquet file formats support various compression algorithms, such as Snappy, Gzip, and LZ4, resulting in smaller file sizes, compared to uncompressed formats like CSV. You can expect a size reduction of nearly 75% for your data in Parquet files from other formats. -->

<!--     Embedded metadata -->

<!-- Parquet file formats include metadata that provide information about the schema, compression settings, number of values, location of columns, minimum value, maximum value, number of row groups and type of encoding.  -->

<!-- Embedded metadata helps in efficiently reading and processing the data. Any program that's used to read the data can also access the metadata to determine what type of data is expected to be found in a given column. -->

## Performance

As opposed to row-based file formats like CSV, Parquet is optimized for performance. When running queries on your Parquet-based file-system, you can focus only on the relevant data very quickly. Moreover, the amount of data scanned will be way smaller and will result in less I/O usage. To understand this, let's look a bit deeper into how Parquet files are structured.

As we mentioned above, Parquet is a self-described format, so each file contains both data and metadata. Parquet files are composed of row groups, header and footer. Each row group contains data from the same columns. The same columns are stored together in each row group:

![](https://www.upsolver.com/wp-content/uploads/2020/05/Screen-Shot-2020-05-26-at-17.53.13.png)
This structure is well-optimized both for fast query performance, as well as low I/O (minimizing the amount of data scanned). For example, if you have a table with 1000 columns, which you will usually only query using a small subset of columns. Using Parquet files will enable you to fetch only the required columns and their values, load those in memory and answer the query. If a row-based file format like CSV was used, the entire table would have to have been loaded in memory, resulting in increased I/O and worse performance.

<!--     Splittable and parallel processing -->

<!-- Parquet file formats are designed to be splittable, meaning they can be divided into smaller chunks for parallel processing in distributed computing frameworks like Apache Hadoop and Apache Spark. -->
<!-- Parquet file format vs CSV -->

<!-- While CSV is widely used in major organizations, CSV and Parquet file formats are suitable for different use cases. Let's look at the differences between these two specific formats in order to help you choose a data storage format. -->

<!--     Storage efficiency -->

<!-- Parquet file format is a columnar storage format, which means that data for each column is stored together. The storage mechanism enables better compression and typically results in smaller file sizes compared to row-based formats. -->

<!-- CSV is a row-based format, where each row is represented as a separate line in the file. The format does not offer compression, often resulting in larger file sizes. -->

<!--     Query performance -->

<!-- CSVs need you to read the entire file to query just one column, which is highly inefficient. -->

<!-- On the other hand, Parquet's columnar storage and efficient compression makes it well-suited for analytical queries that only need to access specific columns. This compression leads to faster query performance when dealing with large datasets. A  recent [survey by Green Shield Canada](https://posit.co/blog/speed-up-data-analytics-with-parquet-files/){target="_blank"} found that with the parquet file format, they were able to process and query data 1,500 times faster than with CSVs.  -->

## Schema evolution

When using columnar file formats like Parquet, users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. In these cases, Parquet supports automatic schema merging among these files.

<!--     Schema evolution -->

<!-- Parquet file format supports schema evolution by default, since it's designed with the dynamic nature of computer systems in mind. The format allows you to add new columns of data without having to worry about your existing dataset.  -->

<!-- CSV files on the other hand, do not inherently support schema evolution, which can be a limitation if your data schema changes frequently. -->

:::

## Column-Oriented vs Row-Based Storage for Analytic Querying

Data is often generated and more easily conceptualized in rows. We are used to thinking in terms of Excel spreadsheets, where we can see all the data relevant to a specific record in one neat and organized row. However, for large-scale analytical querying, columnar storage comes with significant advantages with regards to cost and performance.

Complex data such as logs and event streams would need to be represented as a table with hundreds or thousands of columns, and many millions of rows. Storing this table in a row based format such as CSV would mean:

* Queries will take longer to run since more data needs to be scanned, rather than only querying the subset of columns we need to answer a query (which typically requires aggregating based on dimension or category)
* Storage will be more costly since CSVs are not compressed as efficiently as Parquet

Columnar formats provide better compression and improved performance out-of-the-box, and enable you to query data vertically – column by column. <!-- https://www.upsolver.com/blog/apache-parquet-why-use -->

### Advantages of Storing Data in a Columnar Format:

    Columnar storage like Apache Parquet is designed to bring efficiency compared to row-based files like CSV. When querying, columnar storage you can skip over the non-relevant data very quickly. As a result, aggregation queries are less time-consuming compared to row-oriented databases. This way of storage has translated into hardware savings and minimized latency for accessing data.
    Apache Parquet is built from the ground up. Hence it is able to support advanced nested data structures. The layout of Parquet data files is optimized for queries that process large volumes of data, in the gigabyte range for each individual file.
    Parquet is built to support flexible compression options and efficient encoding schemes. As the data type for each column is quite similar, the compression of each column is straightforward (which makes queries even faster). Data can be compressed by using one of the several codecs available; as a result, different data files can be compressed differently.
    Apache Parquet works best with interactive and serverless technologies like AWS Athena, Amazon Redshift Spectrum, Google BigQuery and Google Dataproc.
    
<!-- https://coralogix.com/blog/parquet-file-format/ -->
















